{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Voxceleb Speaker Verification Task**\n",
    "- train model using Voxceleb1 dataset, test model using Voxceleb2 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "from datetime import datetime\n",
    "import math\n",
    "from IPython.display import display, clear_output\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from sklearn import metrics\n",
    "from operator import itemgetter\n",
    "import math\n",
    "import random\n",
    "import os \n",
    "DEV = \"cuda\"\n",
    "\n",
    "#print gpu info\n",
    "print(\"GPU availalbe :\", torch.cuda.is_available())\n",
    "print(\"GPU name :\", torch.cuda.get_device_name(0))\n",
    "print(\"GPU count :\", torch.cuda.device_count())\n",
    "\n",
    "VOX1_DIR = Path('voxceleb1_full_dataset/')\n",
    "VOX2_DIR = Path('voxceleb_dataset/voxceleb2_dev_iip/voxceleb2_dev_iip/')\n",
    "\n",
    "\n",
    "sampling_rate = 16000 #Hz\n",
    "max_audio_sec = 2.0\n",
    "max_audio_length = int(max_audio_sec * sampling_rate + 240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Data loader**\n",
    "- On the fly method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoxcelebDataset:\n",
    "    def __init__(self, dir_path, max_audio_length, split='train', sr=16000):\n",
    "        self.dir = Path(dir_path)\n",
    "        self.sr = sr #sampling rate\n",
    "        self.max_audio_length = max_audio_length #audio maximum length\n",
    "        #if split==\"test\":\n",
    "        #    self.labels = pd.read_table(self.dir/(str(self.dir).split('\\\\')[-1]+\".txt\"), sep=' ', header=None)\n",
    "        \n",
    "        \n",
    "    def convert_label_to_tensor(self):\n",
    "        return torch.LongTensor(self.labels.values[:,0].astype('bool'))\n",
    "        \n",
    "    def trim_pad_audio(self, audio, random_trim=False):\n",
    "        if audio.shape[0] <= self.max_audio_length: #길이 부족할시 뒤에 zero-padding\n",
    "            pad_size = max(0, self.max_audio_length - audio.shape[0])\n",
    "            audio = F.pad(audio, (0, pad_size), 'constant', 0)\n",
    "        else:\n",
    "            if random_trim == True: #랜덤한 시작점에서부터 자르기\n",
    "                st = np.int64(random.random()*(audio.shape[0]-self.max_audio_length))\n",
    "                audio = audio[st:st+self.max_audio_length]\n",
    "            else:\n",
    "                audio = audio[:self.max_audio_length] #아닐시 시작점부터 자르기\n",
    "            \n",
    "        return audio\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainValidLoader(VoxcelebDataset):\n",
    "    def __init__(self, dir_path, max_audio_length, split, sr=16000):\n",
    "        super().__init__(dir_path, max_audio_length, split, sr)\n",
    "        self.split = split\n",
    "        \n",
    "        if split==\"train\":\n",
    "            self.dir = self.dir/\"wav_train\"\n",
    "            speaker_ids = list(set([d.name for d in self.dir.iterdir() if d.is_dir()]))\n",
    "            speaker_ids.sort()\n",
    "            self.train_speaker_ids = speaker_ids\n",
    "\n",
    "            #one-hot encoding 위한 speaker_id:idx 매칭\n",
    "            speaker_id_to_idx = {speaker_id:idx for idx, speaker_id in enumerate(self.train_speaker_ids)}\n",
    "            indexed_speaker_id = [speaker_id_to_idx[s] for s in self.train_speaker_ids]\n",
    "            self.mapping_dict = {self.train_speaker_ids[k]:indexed_speaker_id[k] for k in range(len(indexed_speaker_id))}\n",
    "            \n",
    "            #wav_train에서 모든 오디오 경로 획득\n",
    "            self.audio_path_list = self.find_wav_files(self.dir)\n",
    "            self.labels = self.audio_path_list #__len__ return 용도. 다른 용도는 없음\n",
    "            \n",
    "        elif split == \"valid\":\n",
    "            self.labels = pd.read_table(self.dir/\"veri_test2.txt\", sep=' ', header=None)\n",
    "            self.label_tensor = self.convert_label_to_tensor() #train은 labels 필요없음\n",
    "            \n",
    "            \n",
    "            \n",
    "    def find_wav_files(self, base_path):\n",
    "        wav_files = []\n",
    "        for dirpath, dirnames, filenames in os.walk(base_path):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.wav'):\n",
    "                    full_path = os.path.join(dirpath, filename)\n",
    "                    relative_path = os.path.relpath(full_path, base_path)\n",
    "                    slash_path = relative_path.replace('\\\\', '/')\n",
    "                    wav_files.append(slash_path)\n",
    "\n",
    "        return wav_files\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.split == \"train\":\n",
    "            file_path = self.dir/self.audio_path_list[idx]\n",
    "            waveform, orig_sr = torchaudio.load(file_path)\n",
    "            audio = torchaudio.functional.resample(waveform, orig_freq=orig_sr, new_freq=self.sr)[0]\n",
    "            audio = self.trim_pad_audio(audio, random_trim=True)\n",
    "            \n",
    "            label = self.mapping_dict[self.audio_path_list[idx].split('/')[0]]\n",
    "            #label = label\n",
    "            \n",
    "            return audio, label\n",
    "        \n",
    "        elif self.split == \"valid\":\n",
    "            file_path = self.dir/\"wav_test\"/self.labels.iloc[idx][1]\n",
    "            waveform, orig_sr = torchaudio.load(file_path)\n",
    "            audio_1 = torchaudio.functional.resample(waveform, orig_freq=orig_sr, new_freq=self.sr)[0]\n",
    "            \n",
    "            file_path = self.dir/\"wav_test\"/self.labels.iloc[idx][2]\n",
    "            waveform, orig_sr = torchaudio.load(file_path)\n",
    "            audio_2 = torchaudio.functional.resample(waveform, orig_freq=orig_sr, new_freq=self.sr)[0]      \n",
    "            \n",
    "            label = self.label_tensor[idx].type(torch.FloatTensor)\n",
    "            \n",
    "            return audio_1, audio_2, label\n",
    "\n",
    "        \n",
    "\n",
    "class TestLoader(VoxcelebDataset):        \n",
    "    def __init__(self, dir_path, max_audio_length, split=\"test\", sr=16000):\n",
    "        super().__init__(dir_path, max_audio_length, split, sr)        \n",
    "        self.labels = pd.read_table(self.dir/(str(self.dir).split('\\\\')[-1]+\".txt\"), sep=' ', header=None)\n",
    "        self.label_tensor = self.convert_label_to_tensor()\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.dir/self.labels.iloc[idx][1]\n",
    "        waveform, orig_sr = torchaudio.load(file_path)\n",
    "\n",
    "        audio_1 = torchaudio.functional.resample(waveform, orig_freq=orig_sr, new_freq=self.sr)[0]\n",
    "\n",
    "        file_path = self.dir/self.labels.iloc[idx][2]\n",
    "        waveform, orig_sr = torchaudio.load(file_path)\n",
    "        audio_2 = torchaudio.functional.resample(waveform, orig_freq=orig_sr, new_freq=self.sr)[0]      \n",
    "\n",
    "        label = self.label_tensor[idx].type(torch.FloatTensor)\n",
    "\n",
    "        return audio_1, audio_2, label\n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "#for train loader test\n",
    "dummy_set = TrainValidLoader(VOX1_DIR, max_audio_length, split=\"train\")\n",
    "audio_1, label = dummy_set[52]\n",
    "ipd.display(ipd.Audio(audio_1, rate=dummy_set.sr))\n",
    "print(label)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "#for validation loader test\n",
    "dummy_set = TrainValidLoader(VOX1_DIR, max_audio_length, split=\"valid\")\n",
    "audio_1, audio_2, label = dummy_set[610]\n",
    "ipd.display(ipd.Audio(audio_1, rate=dummy_set.sr))\n",
    "ipd.display(ipd.Audio(audio_2, rate=dummy_set.sr))\n",
    "print(label)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "#for test loader test\n",
    "dummy_set = TestLoader(VOX2_DIR, max_audio_length)\n",
    "audio_1, audio_2, label = dummy_set[0]\n",
    "ipd.display(ipd.Audio(audio_1, rate=dummy_set.sr))\n",
    "ipd.display(ipd.Audio(audio_2, rate=dummy_set.sr))\n",
    "print(label)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = TrainValidLoader(dir_path=VOX1_DIR, max_audio_length=max_audio_length, split=\"train\")\n",
    "validset = TrainValidLoader(dir_path=VOX1_DIR, max_audio_length=max_audio_length, split=\"valid\")\n",
    "testset = TestLoader(dir_path=VOX2_DIR, max_audio_length=max_audio_length, split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(validset, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(testset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Model Preparation**\n",
    "- SE-Res2 Block, Pre-Emphasis, Log mel-spectrogram, SpecAugment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE_Block(nn.Module):\n",
    "    def __init__(self, in_channels, bottleneck=128):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.conv_1 = nn.Conv1d(in_channels, bottleneck, kernel_size=1, padding=0)\n",
    "        self.conv_2 = nn.Conv1d(bottleneck, in_channels, kernel_size=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv_1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.conv_2(y)\n",
    "        y = self.sigmoid(y)\n",
    "        out = x*y\n",
    "        return out\n",
    "        \n",
    "        \n",
    "class SE_Res2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, k, d, s=8): #k:kernel_size, d:dilation, s:scale_dimension\n",
    "        super().__init__()\n",
    "        self.scale = s\n",
    "        self.width = in_channels//self.scale #res2net 각 element 크기, out_channel%scale==0이여야함.\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv_1 = nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "        self.bn_1 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        convs = []\n",
    "        for i in range(self.scale - 1): #첫번째 split element는 conv 사용을 안해서 scale-1임.\n",
    "            dilation_conv_pad = math.floor(k/2)*d\n",
    "            convs.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(self.width, self.width, kernel_size=k, dilation=d, padding=dilation_conv_pad),\n",
    "                    nn.BatchNorm1d(self.width),\n",
    "                    self.relu,\n",
    "                )\n",
    "            )\n",
    "        self.convs = nn.ModuleList(convs)\n",
    "        \n",
    "        self.conv_3 = nn.Conv1d(out_channels, out_channels, kernel_size=1) #1x1Conv\n",
    "        self.bn_3 = nn.BatchNorm1d(out_channels)\n",
    "        \n",
    "        self.se_block = SE_Block(out_channels, bottleneck=128)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Conv1D + ReLU + BN\n",
    "        x = self.conv_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_1(x)\n",
    "        \n",
    "        #Res2 Dilated Conv1D + ReLU + BN \n",
    "        split_x = torch.split(x, self.width, dim=1)#scale 개로 분할, 각 길이는 width\n",
    "        for idx in range(self.scale): #res2 conv, (https://paperswithcode.com/method/res2net 참조)\n",
    "            if idx==0: #xi\n",
    "                y = split_x[idx] #y[0]\n",
    "                x = y\n",
    "            elif idx==1: #K[i](x[i])\n",
    "                y = self.convs[idx-1](split_x[idx]) #y[1]\n",
    "                x = torch.cat((x,y), dim=1) #stack y[n]\n",
    "            else: #idx>=2, K[i](x[i]+y[i-1])\n",
    "                y = self.convs[idx-1](split_x[idx]+y) # +y는 이전에 연산했던 y, 즉 y[i-1]\n",
    "                x = torch.cat((x,y), dim=1) #stack y[n]\n",
    "        \n",
    "        \n",
    "        #Conv1D + ReLU + BN\n",
    "        x = self.conv_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_3(x)\n",
    "        \n",
    "        #SE-Block\n",
    "        x = self.se_block(x)\n",
    "        \n",
    "        out = x\n",
    "        return out \n",
    "\n",
    "    \n",
    "class PreEmphasis(nn.Module):\n",
    "    def __init__(self, alpha=0.97):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=1, kernel_size=2, stride=1, padding=1, bias=False)\n",
    "        self.conv.weight.data[0, 0, 1] = 1\n",
    "        self.conv.weight.data[0, 0, 0] = -self.alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        emphasized = self.conv(x)\n",
    "        return emphasized[:, :, :-1].squeeze(1)\n",
    "\n",
    "    \n",
    "class LogMelSpec(nn.Module):\n",
    "    #Log-Mel Spectrogram, ECAPA-TDNN 참조\n",
    "    def __init__(self, sr, n_fft, hop_length, n_mels):\n",
    "        super().__init__()\n",
    "        #window_length=sr*window_ms, hop_length=sr*frame_shift\n",
    "        self.mel_converter = torchaudio.transforms.MelSpectrogram(sample_rate=sr, n_fft=n_fft, win_length=400, \n",
    "                                                                  hop_length=hop_length, f_min=20, f_max=7600,\n",
    "                                                                  window_fn=torch.hamming_window, n_mels=n_mels)\n",
    "    def forward(self, x):\n",
    "        mel_spec = self.mel_converter(x)\n",
    "        log_mel_spec = (mel_spec + 1e-6).log() #log(0) 방지\n",
    "        return log_mel_spec\n",
    "\n",
    "    \n",
    "class SpecAugment(nn.Module):\n",
    "    def __init__(self, freq_mask_range=(0,10), time_mask_range=(0,5)): #range는 ECAPA-TDNN의 값을 따름\n",
    "        super().__init__()\n",
    "        #masking 범위 설정시 uniform distribution(random)에 의해 선택되어야함.\n",
    "        self.freq_mask_range = freq_mask_range\n",
    "        self.time_mask_range = time_mask_range\n",
    "           \n",
    "    def frequency_masking(self, x):#dim=1\n",
    "        freq_mask_range = self.freq_mask_range\n",
    "        batch_size, n_mels, time = x.shape\n",
    "        freq_mask_len = torch.randint(freq_mask_range[0], freq_mask_range[1], (batch_size, ), device=x.device)\n",
    "        freq_mask_pos = torch.randint(0, max(1, n_mels - freq_mask_len.max()), (batch_size, ), device=x.device)\n",
    "        for i in range(batch_size):\n",
    "            x[i,freq_mask_pos[i]:freq_mask_pos[i]+freq_mask_len[i],:] = 0\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def time_masking(self, x):#dim=2\n",
    "        time_mask_range = self.time_mask_range\n",
    "        batch_size, n_mels, time = x.shape\n",
    "        time_mask_len = torch.randint(time_mask_range[0], time_mask_range[1], (batch_size, ), device=x.device)\n",
    "        time_mask_pos = torch.randint(0, max(1, time - time_mask_len.max()), (batch_size, ), device=x.device)\n",
    "        for i in range(batch_size):\n",
    "            x[i,:,time_mask_pos[i]:time_mask_pos[i]+time_mask_len[i]] = 0\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.frequency_masking(x)\n",
    "        x = self.time_masking(x)\n",
    "        #time wrapping은 딱히 큰 영향이 없음.\n",
    "        return x\n",
    "    \n",
    "\n",
    "class AttentiveStatPool(nn.Module):\n",
    "    def __init__(self, in_channels, bottleneck=64):\n",
    "        super().__init__()\n",
    "        #x의 통계 정보를 활용하는 것으로 1d conv에서 channel + context까지 고려 가능.\n",
    "        #SE block을 attention으로 사용하면 AdaptiveAvgPool때문에 Context 정보에 손실 발생함. Pooling을 제거하고 변형해야함.\n",
    "        self.conv_1 = nn.Conv1d(in_channels*3, bottleneck, kernel_size=1, padding=0)\n",
    "        self.conv_2 = nn.Conv1d(bottleneck, in_channels, kernel_size=1, padding=0)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=2) #time 축에 대해서 softmax\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Global properties of the utterance (x, mean(x), sd(x)) , shape : (batch_size,1536,T) -> (batch_size,1536*3,T)\n",
    "        T = x.shape[-1]\n",
    "        glob_prop_utt_x = torch.cat(\n",
    "            (x, torch.mean(x, dim=2, keepdim=True).repeat(1,1,T), torch.sqrt(torch.var(x,dim=2,keepdim=True).clamp(min=1e-4)).repeat(1,1,T)), \n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        #ASP weight(or attention)\n",
    "        w = self.conv_1(glob_prop_utt_x)\n",
    "        w = self.relu(w)\n",
    "        w = self.conv_2(w)\n",
    "        w = self.softmax(w) #shape : (batch_size,1536,T)\n",
    "        \n",
    "        #Weighted Statistics Pooling\n",
    "        weighted_mean_x = torch.sum(x*w,dim=2) #평균값인데 데이터 개수 모두 같아서 굳이 안나눠줘도됨\n",
    "        weighted_sd_x = torch.sqrt((torch.sum((x**2)*w, dim=2)-weighted_mean_x**2).clamp(min=1e-4)) #제곱*가중치의 평균 - 가중평균의 제곱\n",
    "        \n",
    "        x = torch.cat((weighted_mean_x, weighted_sd_x), dim=1)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, sr, n_fft, hop_length, n_mels, C):\n",
    "        super().__init__()\n",
    "        self.sr = sr\n",
    "        \n",
    "        self.relu = nn.ReLU() \n",
    "        \n",
    "        self.pre_emphasis = PreEmphasis(alpha=0.97)\n",
    "        self.log_mel_spec = LogMelSpec(sr, n_fft, hop_length, n_mels)\n",
    "        self.spec_augment = SpecAugment()\n",
    "        \n",
    "        self.conv_1 = nn.Conv1d(n_mels, C, kernel_size=5, stride=1, padding=2, dilation=1) #T 유지위해 padding\n",
    "        self.bn_1 = nn.BatchNorm1d(C)\n",
    "        \n",
    "        self.se_res2_1 = SE_Res2Block(C, C, k=3, d=2)\n",
    "        self.se_res2_2 = SE_Res2Block(C, C, k=3, d=3)\n",
    "        self.se_res2_3 = SE_Res2Block(C, C, k=3, d=4)\n",
    "        \n",
    "        self.conv_2 = nn.Conv1d(3*C, int(3*C/2), kernel_size=1, dilation=1) #1536xT\n",
    "        self.attentive_stat_pool = AttentiveStatPool(int(3*C/2))\n",
    "        self.bn_2 = nn.BatchNorm1d(3*C) #3072x1\n",
    "        \n",
    "        self.fc_1 = nn.Linear(3*C,192)#int(3*C/16)) #192x1\n",
    "        self.bn_3 = nn.BatchNorm1d(192)#int(3*C/16))\n",
    "        \n",
    "    def forward(self, x, spec_aug=False):\n",
    "        with torch.no_grad():\n",
    "            x = self.pre_emphasis(x)\n",
    "            x = self.log_mel_spec(x)\n",
    "            x = x - torch.mean(x, dim=-1, keepdim=True)\n",
    "            if(spec_aug==True):\n",
    "                x = self.spec_augment(x)      \n",
    "        #Conv1D + ReLU + BN (k=5, d=1) , shape:(C,T)\n",
    "        x = self.conv_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_1(x)\n",
    "        \n",
    "        #SE-Res2Block (k=3, d=2) , shape:(C,T)\n",
    "        x_1 = self.se_res2_1(x) #skip connection 사용시 결과가 더 좋았다고 conclusion에 있음\n",
    "        #SE-Res2Block (k=3, d=3) , shape:(C,T)\n",
    "        x_2 = self.se_res2_2(x+x_1)\n",
    "        #SE-Res2Block (k=3, d=4) , shape:(C,T)\n",
    "        x_3 = self.se_res2_3(x+x_1+x_2)\n",
    "        \n",
    "        #Conv1D + ReLU (k=1, d=1) , shape:(1536,T)\n",
    "        x = torch.cat((x_1,x_2,x_3), dim=1) #3x(C*T)개 합치기\n",
    "        x = self.conv_2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        #Attentive Stat Pooling + BN , shape:(3072x1)\n",
    "        x = self.attentive_stat_pool(x)\n",
    "        x = self.bn_2(x)\n",
    "        \n",
    "        #FC + BN , shape:(192,1)\n",
    "        x = self.fc_1(x)\n",
    "        x = self.bn_3(x)\n",
    "\n",
    "        out = x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Evaluation function, Loss function**\n",
    "- Evaluation function : EER, minDCF\n",
    "- Loss function : AAM-Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target, topk=(1,)):\n",
    "\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AAMsoftmax(nn.Module):\n",
    "    def __init__(self, n_class=100, m=0.3, s=15.0, embed_size=48):\n",
    "        \n",
    "        super(AAMsoftmax, self).__init__()\n",
    "        self.m = m\n",
    "        self.s = s\n",
    "        self.weight = torch.nn.Parameter(torch.FloatTensor(n_class, embed_size), requires_grad=True)\n",
    "        self.ce = nn.CrossEntropyLoss()\n",
    "        nn.init.xavier_normal_(self.weight, gain=1)\n",
    "        self.cos_m = math.cos(self.m)\n",
    "        self.sin_m = math.sin(self.m)\n",
    "        self.th = math.cos(math.pi - self.m)\n",
    "        self.mm = math.sin(math.pi - self.m) * self.m\n",
    "\n",
    "    def forward(self, x, label=None):\n",
    "        \n",
    "        cosine = F.linear(F.normalize(x), F.normalize(self.weight))\n",
    "        sine = torch.sqrt((1.0 - torch.mul(cosine, cosine)).clamp(0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        phi = torch.where((cosine - self.th) > 0, phi, cosine - self.mm)\n",
    "        one_hot = torch.zeros_like(cosine)\n",
    "        one_hot.scatter_(1, label.view(-1, 1), 1)\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n",
    "        output = output * self.s\n",
    "        \n",
    "        loss = self.ce(output, label)\n",
    "        prec1 = accuracy(output.detach(), label.detach(), topk=(1,))[0]\n",
    "        return loss, prec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuneThresholdfromScore(scores, labels, target_fa, target_fr = None):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(labels, scores, pos_label=1)\n",
    "    fnr = 1 - tpr\n",
    "    tunedThreshold = []\n",
    "    if target_fr:\n",
    "        for tfr in target_fr:\n",
    "            idx = np.nanargmin(np.absolute((tfr - fnr)))\n",
    "            tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n",
    "    for tfa in target_fa:\n",
    "        idx = np.nanargmin(np.absolute((tfa - fpr))) # np.where(fpr<=tfa)[0][-1]\n",
    "        tunedThreshold.append([thresholds[idx], fpr[idx], fnr[idx]])\n",
    "    idxE = np.nanargmin(np.absolute((fnr - fpr)))\n",
    "    eer  = max(fpr[idxE],fnr[idxE])*100\n",
    "\n",
    "    return tunedThreshold, eer, fpr, fnr\n",
    "\n",
    "# Creates a list of false-negative rates, a list of false-positive rates\n",
    "# and a list of decision thresholds that give those error-rates.\n",
    "def ComputeErrorRates(scores, labels):\n",
    "    # Sort the scores from smallest to largest, and also get the corresponding\n",
    "    # indexes of the sorted scores.  We will treat the sorted scores as the\n",
    "    # thresholds at which the the error-rates are evaluated.\n",
    "    sorted_indexes, thresholds = zip(*sorted(\n",
    "        [(index, threshold) for index, threshold in enumerate(scores)],\n",
    "        key=itemgetter(1)))\n",
    "    sorted_labels = []\n",
    "    labels = [labels[i] for i in sorted_indexes]\n",
    "    fnrs = []\n",
    "    fprs = []\n",
    "\n",
    "    # At the end of this loop, fnrs[i] is the number of errors made by\n",
    "    # incorrectly rejecting scores less than thresholds[i]. And, fprs[i]\n",
    "    # is the total number of times that we have correctly accepted scores\n",
    "    # greater than thresholds[i].\n",
    "    for i in range(0, len(labels)):\n",
    "        if i == 0:\n",
    "            fnrs.append(labels[i])\n",
    "            fprs.append(1 - labels[i])\n",
    "        else:\n",
    "            fnrs.append(fnrs[i-1] + labels[i])\n",
    "            fprs.append(fprs[i-1] + 1 - labels[i])\n",
    "    fnrs_norm = sum(labels)\n",
    "    fprs_norm = len(labels) - fnrs_norm\n",
    "\n",
    "    # Now divide by the total number of false negative errors to\n",
    "    # obtain the false positive rates across all thresholds\n",
    "    fnrs = [x / float(fnrs_norm) for x in fnrs]\n",
    "\n",
    "    # Divide by the total number of corret positives to get the\n",
    "    # true positive rate.  Subtract these quantities from 1 to\n",
    "    # get the false positive rates.\n",
    "    fprs = [1 - x / float(fprs_norm) for x in fprs]\n",
    "    return fnrs, fprs, thresholds\n",
    "\n",
    "# Computes the minimum of the detection cost function.  The comments refer to\n",
    "# equations in Section 3 of the NIST 2016 Speaker Recognition Evaluation Plan.\n",
    "def ComputeMinDcf(fnrs, fprs, thresholds, p_target, c_miss, c_fa):\n",
    "    min_c_det = float(\"inf\")\n",
    "    min_c_det_threshold = thresholds[0]\n",
    "    for i in range(0, len(fnrs)):\n",
    "        # See Equation (2).  it is a weighted sum of false negative\n",
    "        # and false positive errors.\n",
    "        c_det = c_miss * fnrs[i] * p_target + c_fa * fprs[i] * (1 - p_target)\n",
    "        if c_det < min_c_det:\n",
    "            min_c_det = c_det\n",
    "            min_c_det_threshold = thresholds[i]\n",
    "    # See Equations (3) and (4).  Now we normalize the cost.\n",
    "    c_def = min(c_miss * p_target, c_fa * (1 - p_target))\n",
    "    min_dcf = min_c_det / c_def\n",
    "    return min_dcf, min_c_det_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Train model**\n",
    "- Train/validate model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, valid_loader, optimizer, num_epochs, loss_func, device=\"cuda\"):\n",
    "    loss_records = []\n",
    "    valid_acc_records = []\n",
    "    cur_valid_acc = []\n",
    "    \n",
    "    model.train()\n",
    "    index, top1, loss = 0,0,0\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            audio, label = batch\n",
    "            audio, label = audio.to(device), label.to(device)\n",
    "            \n",
    "            speaker_embedding = model(audio, spec_aug=True)\n",
    "            \n",
    "            nloss, prec = loss_func(speaker_embedding, label)\n",
    "            nloss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            index += len(label)\n",
    "            top1 += prec\n",
    "            loss += nloss.detach().cpu().numpy()\n",
    "            loss_records.append(nloss.item())\n",
    "            \n",
    "            #draw loss graph\n",
    "            clear_output(wait=True)  # 이전 그래프를 지웁니다.\n",
    "            plt.plot(np.array(loss_records))\n",
    "            plot_title = 'Train Loss:'+str(nloss.item())+\" Val Loss:\"+str(cur_valid_acc)+\"ACC:\"+str(top1/index*len(label))\n",
    "            plt.title(plot_title)#\n",
    "            plt.show()\n",
    "            display(tqdm(range(num_epochs), position=0, leave=True))\n",
    "        \n",
    "        EER, minDCF = validate_model(model, valid_loader, device)\n",
    "        cur_valid_acc = [EER, minDCF]\n",
    "        valid_acc_records.append([EER, minDCF])\n",
    "        save_model(model, optimizer, {\"loss\": loss_records, \"valid_acc\": valid_acc_records}, epoch)\n",
    "    \n",
    "    return {\"loss\": loss_records, \"valid_acc\": valid_acc_records}\n",
    "\n",
    "\n",
    "def valid_audio_processing(model, audio, device):\n",
    "    audio = audio.squeeze(0)\n",
    "    if audio.shape[0] <= max_audio_length:\n",
    "        pad_size = max(0, self.max_audio_length - audio.shape[0]) \n",
    "        audio = torch.nn.functional.pad(audio, (0, pad_size), 'constant', 0)\n",
    "        \n",
    "    audio = audio.detach().cpu().numpy()\n",
    "    data_1 = torch.FloatTensor(np.stack([audio],axis=0)).to(device)\n",
    "    max_audio = max_audio_length\n",
    "\n",
    "    feats = []\n",
    "    startframe = np.linspace(0, audio.shape[0]-max_audio, num=5)\n",
    "\n",
    "    for asf in startframe:\n",
    "        feats.append(audio[int(asf):int(asf)+max_audio])\n",
    "\n",
    "\n",
    "    feats = np.stack(feats, axis = 0).astype(np.float64)\n",
    "    data_2 = torch.FloatTensor(feats).to(device)\n",
    "    # Speaker embeddings\n",
    "\n",
    "    embedding_1 = model(data_1)\n",
    "    embedding_1 = F.normalize(embedding_1, p=2, dim=1)\n",
    "    embedding_2 = model(data_2)\n",
    "    embedding_2 = F.normalize(embedding_2, p=2, dim=1)\n",
    "    return [embedding_1, embedding_2]\n",
    "\n",
    "\n",
    "def validate_model(model, valid_loader, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    scores, labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in valid_loader:\n",
    "            audio_1, audio_2, label = batch\n",
    "            audio_1, audio_2, label = audio_1.to(device), audio_2.to(device), label.to(device)\n",
    "            \n",
    "            embeddings_1 = valid_audio_processing(model, audio_1, device)\n",
    "            embeddings_2 = valid_audio_processing(model, audio_2, device)\n",
    "            \n",
    "            embedding_11, embedding_12 = embeddings_1\n",
    "            embedding_21, embedding_22 = embeddings_2\n",
    "\n",
    "            # Compute the scores\n",
    "            score_1 = torch.mean(torch.matmul(embedding_11, embedding_21.T)) # higher is positive\n",
    "            score_2 = torch.mean(torch.matmul(embedding_12, embedding_22.T))\n",
    "            score = (score_1 + score_2) / 2\n",
    "            score = score.detach().cpu().numpy()\n",
    "            scores.append(score)\n",
    "            labels.append(label.squeeze(0).detach().cpu().numpy())\n",
    "    \n",
    "    EER = tuneThresholdfromScore(scores, labels, [1, 0.1])[1]\n",
    "    fnrs, fprs, thresholds = ComputeErrorRates(scores, labels)\n",
    "    minDCF, _ = ComputeMinDcf(fnrs, fprs, thresholds, 0.05, 1, 1) \n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return EER, minDCF\n",
    "\n",
    "\n",
    "def save_model(model, optimizer, train_record, epoch):\n",
    "    torch.save({\n",
    "    'model_state_dict':model.state_dict(),\n",
    "    'optimizer_state_dict':optimizer.state_dict(),\n",
    "    'train_record':train_record\n",
    "    }, './save_models/model_epoch_'+str(epoch)+'.pth')\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(sr=sampling_rate, n_fft=512, hop_length=160, n_mels=80, C=512)\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=2e-5)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
    "model = model.to(DEV)\n",
    "\n",
    "loss_func = AAMsoftmax(n_class=len(trainset.train_speaker_ids), m=0.3, s=15.0, embed_size=192).to(DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record = train_model(model, train_loader, valid_loader, optimizer, num_epochs=80, \n",
    "                           loss_func=loss_func, device=DEV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(train_record[\"valid_acc\"])[:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iip_test_lv2",
   "language": "python",
   "name": "iip_test_lv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
